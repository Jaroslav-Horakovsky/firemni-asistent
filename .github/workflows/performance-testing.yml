# =============================================================================
# AUTOMATED PERFORMANCE TESTING PIPELINE
# =============================================================================
# Comprehensive performance testing using Playwright + Browser-tools integration
# Runs on staging deployments and PR validation with baseline comparison

name: Performance Testing Pipeline

on:
  # Trigger on staging deployments
  workflow_run:
    workflows: ["Multi-Environment Deployment Pipeline"]
    types: [completed]
    branches: [staging]
  
  # Manual trigger for performance validation
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment for performance testing'
        required: true
        default: 'staging'
        type: choice
        options:
          - dev
          - staging
          - prod
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'full'
        type: choice
        options:
          - smoke
          - full
          - load
          - stress
      create_baseline:
        description: 'Create new performance baseline'
        required: false
        default: false
        type: boolean

env:
  # Node.js Configuration
  NODE_VERSION: '20'
  
  # Performance Testing Configuration  
  TEST_TIMEOUT: '300000'  # 5 minutes
  PERFORMANCE_THRESHOLD_INCREASE: '15'  # 15% regression threshold
  
  # Browser-tools Configuration
  BROWSER_TOOLS_SERVER_PORT: '3025'
  BROWSER_TOOLS_TIMEOUT: '30000'
  
  # Environment URLs
  DEV_BASE_URL: 'https://dev.firemni-asistent.cz'
  STAGING_BASE_URL: 'https://staging.firemni-asistent.cz'
  PROD_BASE_URL: 'https://firemni-asistent.cz'

jobs:
  # =============================================================================
  # SETUP AND VALIDATION
  # =============================================================================
  setup-performance-testing:
    name: Setup Performance Testing Environment
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.setup.outputs.environment }}
      base-url: ${{ steps.setup.outputs.base-url }}
      test-type: ${{ steps.setup.outputs.test-type }}
      should-run: ${{ steps.setup.outputs.should-run }}
    
    steps:
      - name: Determine Test Configuration
        id: setup
        run: |
          # Determine environment and test type
          if [[ "${{ github.event_name }}" == "workflow_run" ]]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
            echo "base-url=${{ env.STAGING_BASE_URL }}" >> $GITHUB_OUTPUT
            echo "test-type=full" >> $GITHUB_OUTPUT
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "🎯 Triggered by staging deployment - running full performance tests"
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
            echo "test-type=${{ github.event.inputs.test_type }}" >> $GITHUB_OUTPUT
            echo "should-run=true" >> $GITHUB_OUTPUT
            
            case "${{ github.event.inputs.environment }}" in
              dev)
                echo "base-url=${{ env.DEV_BASE_URL }}" >> $GITHUB_OUTPUT
                ;;
              staging)
                echo "base-url=${{ env.STAGING_BASE_URL }}" >> $GITHUB_OUTPUT
                ;;
              prod)
                echo "base-url=${{ env.PROD_BASE_URL }}" >> $GITHUB_OUTPUT
                ;;
            esac
            
            echo "🎯 Manual trigger - testing ${{ github.event.inputs.environment }} with ${{ github.event.inputs.test_type }} tests"
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
            echo "❌ Unsupported trigger event"
          fi
      
      - name: Validate Target Environment
        if: steps.setup.outputs.should-run == 'true'
        run: |
          echo "🔍 Validating target environment..."
          
          # Health check target environment
          if ! curl -f --max-time 30 "${{ steps.setup.outputs.base-url }}/health"; then
            echo "❌ Target environment health check failed"
            echo "🔗 URL: ${{ steps.setup.outputs.base-url }}/health"
            exit 1
          fi
          
          echo "✅ Target environment is healthy and ready for testing"
          echo "🔗 Base URL: ${{ steps.setup.outputs.base-url }}"
          echo "🧪 Test Type: ${{ steps.setup.outputs.test-type }}"
          echo "🌍 Environment: ${{ steps.setup.outputs.environment }}"

  # =============================================================================
  # PERFORMANCE TESTING EXECUTION
  # =============================================================================
  run-performance-tests:
    name: Execute Performance Tests
    runs-on: ubuntu-latest
    needs: setup-performance-testing
    if: needs.setup-performance-testing.outputs.should-run == 'true'
    
    strategy:
      matrix:
        test-suite:
          - core-user-journey
          - api-performance
          - mobile-performance
      fail-fast: false
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install Root Dependencies
        run: npm ci
      
      - name: Install Performance Test Dependencies
        run: |
          cd tests/performance
          npm ci
      
      - name: Install Playwright Browsers
        run: |
          cd tests/performance
          npx playwright install --with-deps chromium firefox webkit
      
      - name: Setup Browser-tools Server
        run: |
          echo "🚀 Starting browser-tools server..."
          
          # Use centralized bt start command
          if command -v bt &> /dev/null; then
            bt start
          else
            echo "⚠️  bt command not found, using fallback method"
            # Fallback to direct server startup (if bt alias not available in CI)
            nohup browser-tools-server --port=${{ env.BROWSER_TOOLS_SERVER_PORT }} --connector > browser-tools.log 2>&1 &
            echo $! > browser-tools.pid
          fi
          
          # Wait for server to be ready
          timeout 60 bash -c '
            until curl -f http://localhost:${{ env.BROWSER_TOOLS_SERVER_PORT }}/health 2>/dev/null; do
              echo "Waiting for browser-tools server..."
              sleep 2
            done
          '
          
          echo "✅ Browser-tools server is ready"
      
      - name: Create Performance Baseline (if requested)
        if: github.event.inputs.create_baseline == 'true'
        working-directory: tests/performance
        run: |
          echo "📊 Creating new performance baseline..."
          npm run baseline:create
        env:
          TEST_BASE_URL: ${{ needs.setup-performance-testing.outputs.base-url }}
          TEST_ENV: ${{ needs.setup-performance-testing.outputs.environment }}
      
      - name: Execute Performance Tests
        working-directory: tests/performance
        run: |
          echo "🧪 Running ${{ matrix.test-suite }} performance tests..."
          
          case "${{ needs.setup-performance-testing.outputs.test-type }}" in
            smoke)
              npm run test -- --grep="smoke" --project="${{ matrix.test-suite }}"
              ;;
            load)
              npm run load-test
              ;;
            stress)
              npm run stress-test
              ;;
            full|*)
              npm run test -- --project="${{ matrix.test-suite }}"
              ;;
          esac
        env:
          TEST_BASE_URL: ${{ needs.setup-performance-testing.outputs.base-url }}
          TEST_ENV: ${{ needs.setup-performance-testing.outputs.environment }}
          TEST_USER_EMAIL: ${{ secrets.TEST_USER_EMAIL }}
          TEST_USER_PASSWORD: ${{ secrets.TEST_USER_PASSWORD }}
          BROWSER_TOOLS_SERVER_URL: http://localhost:${{ env.BROWSER_TOOLS_SERVER_PORT }}
        timeout-minutes: 15
      
      - name: Analyze Performance Results
        if: always()
        working-directory: tests/performance
        run: |
          echo "📈 Analyzing performance test results..."
          
          # Compare against baseline if not creating new baseline
          if [[ "${{ github.event.inputs.create_baseline }}" != "true" ]]; then
            npm run baseline:compare
          fi
          
          # Generate comprehensive performance report
          npm run report:generate
          
          echo "✅ Performance analysis completed"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PERFORMANCE_THRESHOLD: ${{ env.PERFORMANCE_THRESHOLD_INCREASE }}
      
      - name: Upload Performance Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ matrix.test-suite }}-${{ needs.setup-performance-testing.outputs.environment }}
          path: |
            tests/performance/test-results/
            tests/performance/reports/
            tests/performance/playwright-report/
          retention-days: 30
      
      - name: Upload Performance Screenshots
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: performance-screenshots-${{ matrix.test-suite }}-${{ needs.setup-performance-testing.outputs.environment }}
          path: tests/performance/test-results/screenshots/
          retention-days: 7
      
      - name: Cleanup Browser-tools Server
        if: always()
        run: |
          echo "🧹 Cleaning up browser-tools server..."
          
          if command -v bt &> /dev/null; then
            bt stop
          else
            # Fallback cleanup
            if [[ -f browser-tools.pid ]]; then
              kill $(cat browser-tools.pid) 2>/dev/null || true
              rm browser-tools.pid
            fi
          fi
          
          echo "✅ Cleanup completed"

  # =============================================================================
  # PERFORMANCE REGRESSION ANALYSIS
  # =============================================================================
  analyze-performance-regression:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [setup-performance-testing, run-performance-tests]
    if: always() && needs.setup-performance-testing.outputs.should-run == 'true'
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Download Performance Results
        uses: actions/download-artifact@v4
        with:
          pattern: performance-results-*
          path: performance-results/
          merge-multiple: true
      
      - name: Install Analysis Dependencies
        run: |
          cd tests/performance
          npm ci
      
      - name: Generate Comprehensive Performance Report
        working-directory: tests/performance
        run: |
          echo "📊 Generating comprehensive performance report..."
          
          # Consolidate results from all test suites
          node scripts/consolidate-results.js ../../performance-results/
          
          # Generate performance regression report
          node scripts/regression-analysis.js
          
          # Generate performance trends report
          node scripts/trends-analysis.js
          
          echo "✅ Performance analysis completed"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PERFORMANCE_THRESHOLD: ${{ env.PERFORMANCE_THRESHOLD_INCREASE }}
      
      - name: Comment Performance Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = './tests/performance/reports/pr-comment.md';
            
            if (fs.existsSync(path)) {
              const comment = fs.readFileSync(path, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
      
      - name: Create Performance Regression Issue
        if: failure() && needs.setup-performance-testing.outputs.environment == 'staging'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read regression analysis results
            const regressionPath = './tests/performance/reports/regression-analysis.json';
            let regressions = [];
            
            if (fs.existsSync(regressionPath)) {
              const data = JSON.parse(fs.readFileSync(regressionPath, 'utf8'));
              regressions = data.regressions || [];
            }
            
            if (regressions.length > 0) {
              const issueBody = `
              ## 🐌 Performance Regression Detected
              
              **Environment:** ${{ needs.setup-performance-testing.outputs.environment }}
              **Commit:** ${context.sha}
              **Workflow:** ${context.workflow}
              
              ### Regression Summary
              ${regressions.map(r => `- **${r.metric}**: ${r.change} (threshold: ${r.threshold})`).join('\n')}
              
              ### Details
              See the [performance test results](${context.payload.repository.html_url}/actions/runs/${context.runId}) for detailed analysis.
              
              ### Action Required
              Please investigate and fix the performance regression before merging.
              `;
              
              github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Performance Regression Detected - ${new Date().toISOString().split('T')[0]}`,
                body: issueBody,
                labels: ['performance', 'regression', 'priority-high']
              });
            }
      
      - name: Upload Comprehensive Performance Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis-report-${{ needs.setup-performance-testing.outputs.environment }}
          path: |
            tests/performance/reports/
            tests/performance/baselines/
          retention-days: 90

  # =============================================================================
  # NOTIFICATION AND CLEANUP
  # =============================================================================
  notify-performance-results:
    name: Notify Performance Test Results
    runs-on: ubuntu-latest
    needs: [setup-performance-testing, run-performance-tests, analyze-performance-regression]
    if: always() && needs.setup-performance-testing.outputs.should-run == 'true'
    
    steps:
      - name: Notify Success
        if: needs.run-performance-tests.result == 'success' && needs.analyze-performance-regression.result == 'success'
        run: |
          echo "✅ Performance tests completed successfully!"
          echo "🌍 Environment: ${{ needs.setup-performance-testing.outputs.environment }}"
          echo "🧪 Test Type: ${{ needs.setup-performance-testing.outputs.test-type }}"
          echo "📊 No performance regressions detected"
          
          # Future: Add Slack/email notification here
      
      - name: Notify Performance Regression
        if: needs.analyze-performance-regression.result == 'failure'
        run: |
          echo "⚠️  Performance regression detected!"
          echo "🌍 Environment: ${{ needs.setup-performance-testing.outputs.environment }}"
          echo "📈 Performance metrics exceeded acceptable thresholds"
          echo "🔍 Review the performance analysis report for details"
          
          # Future: Add critical alert notification here
      
      - name: Notify Failure
        if: needs.run-performance-tests.result == 'failure'
        run: |
          echo "❌ Performance tests failed!"
          echo "🌍 Environment: ${{ needs.setup-performance-testing.outputs.environment }}"
          echo "🧪 Test Type: ${{ needs.setup-performance-testing.outputs.test-type }}"
          echo "🔍 Check test logs and artifacts for failure details"
          
          # Future: Add failure notification here